# REFLECTIVE ESSAY: END OF MODULE ASSIGNMENT

## Table of Contents
1. [WHAT: Module Experience and Learning Activities](#1-what-module-experience-and-learning-activities)
2. [SO WHAT: Emotional Response, Evaluation and Critical Analysis](#2-so-what-emotional-response-evaluation-and-critical-analysis)
3. [NOW WHAT](#3-now-what)
   - [Application to Professional Practice in Cybersecurity](#application-to-professional-practice-in-cybersecurity)
   - [Professional Skills Development and Group Contribution](#professional-skills-development-and-group-contribution)
4. [Conclusion](#4-conclusion)
5. [References](#5-references)

---

## 1. WHAT:  
### Module Experience and Learning Activities  
Throughout this module, I engaged in a diverse range of machine learning (ML) activities, from foundational theory to practical implementation and evaluation. The learning journey spanned individual tasks such as regression modelling, clustering, CNNs, and model evaluation as well as collaborative experiences. This combination of theoretical learning and applied projects enabled me to understand the lifecycle of ML development, from data exploration and model building to performance tuning and ethical considerations aligning with the structured development cycle and common pitfalls outlined by Biderman and Scheirer (2020).

The range of units challenged me to think beyond code execution. They required me to consider the reasoning behind algorithm selection, the implications of bias and variance, and the real-world impact of design decisions. Each artefact in the e-portfolio became a dynamic checkpoint for learning, where concepts were not only implemented but interrogated. As a result, the portfolio became more than a repository of tasks. It evolved into a space for applied thinking and critical reflection, reinforcing a comprehensive understanding of both technical knowledge and its broader contextual relevance. This reflective process was supported by literature such as Pandey et al. (2019), which provided a structured overview of machine learning algorithms and their applications, reinforcing the importance of thoughtful model selection and contextual awareness.

---

## 2. SO WHAT:  
### Emotional Response, Evaluation and Critical Analysis  
Initially, I felt a mix of curiosity and anxiety. While I was excited to explore ML applications through programming, I was also apprehensive about collaborative work, as aligning technical roles in a remote setting can be challenging. However, as our team progressed, my confidence grew. I felt particularly satisfied when we finalised the group project report, it was a solid outcome of our shared efforts and communication. On the individual front, building a CNN from scratch and tuning its performance gave me a real sense of achievement. That said, there were moments of frustration, especially when tackling activities in unit 4 and 7.  
In Unit 4, I struggled to fully understand how unsupervised algorithms like K-Means determined meaningful groupings in the data, especially since there was no clear “correct” answer or labelled output to compare against. It left me unsure whether the clusters generated were actually useful or just arbitrary.  
In Unit 7, while exploring neural networks, I found the conceptual leap from simple perceptron to multi-layered architectures intellectually demanding, especially without writing code myself (Mitra and Chattopadhyay, 2016). These experiences were occasionally demotivating, but they pushed me to engage more deeply with the material and ultimately contributed to my growth.  
These challenges, as explained by Gibbs (1988), were essential reflective triggers: the discomfort led to critical evaluation, peer support, and ultimately a deeper understanding.

The later units involving CNNs (Unit 9) and model evaluation (Unit 11) re-ignited curiosity. Understanding how class imbalance impacts AUC scores or how hyperparameter tuning alters performance became intellectually satisfying. These learnings were validated by the clarity they brought to performance metrics I encounter in my current role. As a cybersecurity analyst in a 24/7 Security Operations Center (SOC), evaluating alert accuracy and precision resonates with model evaluation techniques like confusion matrices and F1 scores (Fawcett, 2006). This connection bridged academic exercises with workplace realities.

---

## 3. NOW WHAT  

### Application to Professional Practice in Cybersecurity  
The skills and insights gained throughout this module have direct and practical implications for my role within a modern Security Operations Center (SOC). Key activities such as implementing classification models, evaluating confusion matrices and F1 scores, and understanding CNN behaviour align with how AI and machine learning are now embedded in cyber defense operations. According to Blaser and Noshi (2024), AI-powered SOCs use ML to enhance anomaly detection, threat intelligence, and automated incident response, significantly improving speed and reducing false positives. These same performance metrics explored during my model evaluation exercises are now tools I intend to apply professionally to assess alert prioritisation and response effectiveness.

Wallace and Ali (2025) emphasise that advanced deep learning models are increasingly shaping the detection and response capabilities within autonomous SOCs. However, their integration also introduces complex challenges. Issues such as model accuracy, data privacy, and ethical concerns must be carefully addressed to ensure effective and trustworthy operations. Reflecting on this, I recognise that while my academic models (e.g., CNNs trained on CIFAR-10) were valuable for learning about layered representations and performance tuning, deploying such models in real SOC environments demands a deeper understanding of explainability, regulatory compliance, and risk accountability, particularly when automating decisions at scale.

Complementing this, Unalp and Pack (2025) advocate for a data-centric approach to cybersecurity, emphasising that the success of ML models depends not only on algorithm design but on the quality and labelling of data. My own experience with training-validation splits, hyperparameter sensitivity, and performance drift under noise directly illustrated these dependencies. Together, these academic perspectives and practical insights have deepened my appreciation for how machine learning can drive SOC efficiency but only when paired with critical oversight, ethical design, and data governance.

### Professional Skills Development and Group Contribution  
Participating in the Unit 6 group project significantly enhanced both my technical and professional skillset. I actively contributed to discussions, particularly in shaping our business question and determining the analytical approach using R. My responsibilities included co-authoring the exploratory data analysis and regression interpretation, as well as drafting the methodology and analysis sections of the final report. These activities strengthened my skills in statistical reasoning, data visualisation, and communicating technical findings clearly. Time management was tested but strengthened through weekly deliverables. I became more adept at structuring my learning, scheduling coding blocks, and balancing this project with full-time work. Independent learning expanded through documentation reviews, tutorials, and experimenting with different Python libraries to compare outcomes. These experiences not only improved my digital fluency but also sharpened my ability to critically assess tools and techniques.  

---

## 4. Conclusion  
Reflecting on this module, I’ve developed a more structured, critical, and practical understanding of machine learning. The Unit 6 group project strengthened my collaborative skills, highlighting the value of communication, shared accountability, and applying statistical techniques to real-world data. In contrast, the CNN-based object recognition task sharpened my coding fluency and deepened my appreciation for model evaluation and the complexity of deep learning. Building the e-portfolio helped consolidate my learning and reflect on each activity. Looking back, I could have engaged earlier with hyperparameter tuning and explored more advanced architectures. I also see how I could have contributed more to model selection in the group project. These experiences have been pivotal in shaping my ability to approach ML systems thoughtfully—both technically and ethically.

---

## 5. References  
- Noshi, A. and Blaser, F., 2024. *Integrating artificial intelligence and machine learning for advanced cyber security in SOC operations*.  
- Ali, N. and Wallace, G., 2025. *The Future of SOC Operations: Autonomous Cyber Defense with AI and Machine Learning*.  
- Pack, P. and Unalp, A., 2025. *Machine Learning-Driven Cyber Security: A Data-Centric Approach to SOC Efficiency*.  
- Biderman, S. and Scheirer, W.J., 2020. *Pitfalls in machine learning research: Reexamining the development cycle*.  
- Fawcett, T., 2006. *An introduction to ROC analysis*. Pattern recognition letters, 27(8), pp.861-874.  
- Mitra, S. and Chattopadhyay, P., 2016, March. *Challenges in implementation of ANN in embedded system*. In 2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT) (pp. 1794-1798). IEEE.  
- Gibbs, G., 1988. *Learning by Doing: A Guide to Teaching and Learning Methods*. Oxford Polytechnic.  
- Pandey, D., Niwaria, K. and Chourasia, B., 2019. *Machine learning algorithms: a review*. Mach. Learn, 6(2).
